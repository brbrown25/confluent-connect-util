use super::config_field;
use crate::types::{ConnectorDefinition, ConnectorType};

// Sink Connectors
pub(crate) fn alloydb_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "AlloyDBSink".to_string(),
        display_name: "AlloyDB Sink".to_string(),
        connector_class: "AlloyDBSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to AlloyDB".to_string(),
        required_configs: vec![
            config_field("alloydb.hostname", "AlloyDB hostname", "string", true, None),
            config_field("alloydb.port", "AlloyDB port", "int", true, None),
            config_field("alloydb.username", "AlloyDB username", "string", true, None),
            config_field("alloydb.database", "AlloyDB database", "string", true, None),
            config_field(
                "alloydb.table.name.format",
                "AlloyDB table name format",
                "string",
                true,
                None,
            ),
        ],
        optional_configs: vec![
            config_field(
                "alloydb.password",
                "AlloyDB password",
                "string",
                false,
                None,
            ),
            config_field(
                "alloydb.ssl.mode",
                "AlloyDB SSL mode",
                "string",
                false,
                Some(vec![
                    "disable".to_string(),
                    "require".to_string(),
                    "verify-ca".to_string(),
                    "verify-full".to_string(),
                    "prefer".to_string(),
                ]),
            ),
            config_field(
                "alloydb.insert.mode",
                "AlloyDB insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "alloydb.pk.mode",
                "AlloyDB primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "alloydb.pk.fields",
                "AlloyDB primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "alloydb.auto.create",
                "AlloyDB auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "alloydb.auto.evolve",
                "AlloyDB auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
        ],
        sensitive_configs: vec!["alloydb.password".to_string()],
    }
}

pub(crate) fn amazon_s3_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "S3_SINK".to_string(),
        display_name: "Amazon S3 Sink".to_string(),
        connector_class: "S3_SINK".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Amazon S3".to_string(),
        required_configs: vec![
            config_field("s3.bucket.name", "S3 bucket name", "string", true, None),
            config_field(
                "topics.dir",
                "Directory prefix for topics in S3",
                "string",
                true,
                None,
            ),
        ],
        optional_configs: vec![
            config_field(
                "s3.wan.mode",
                "Enable WAN mode for S3 access",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "input.data.format",
                "Input data format",
                "string",
                false,
                Some(vec![
                    "AVRO".to_string(),
                    "JSON".to_string(),
                    "PROTOBUF".to_string(),
                    "local.schema_formats.avro".to_string(),
                    "local.schema_formats.json".to_string(),
                    "local.schema_formats.protobuf".to_string(),
                ]),
            ),
            config_field(
                "output.data.format",
                "Output data format",
                "string",
                false,
                Some(vec![
                    "AVRO".to_string(),
                    "JSON".to_string(),
                    "PARQUET".to_string(),
                ]),
            ),
            config_field(
                "time.interval",
                "Time interval for file rotation",
                "string",
                false,
                Some(vec!["HOURLY".to_string(), "DAILY".to_string()]),
            ),
            config_field(
                "rotate.interval.ms",
                "Rotation interval in milliseconds",
                "long",
                false,
                None,
            ),
            config_field(
                "flush.size",
                "Number of records to flush to S3",
                "int",
                false,
                None,
            ),
            config_field(
                "compression.codec",
                "Compression codec",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "PARQUET - gzip".to_string(),
                    "PARQUET - snappy".to_string(),
                    "PARQUET - lz4".to_string(),
                    "JSON - gzip".to_string(),
                    "JSON - snappy".to_string(),
                    "JSON - lz4".to_string(),
                    "PROTOBUF - gzip".to_string(),
                    "PROTOBUF - snappy".to_string(),
                    "PROTOBUF - lz4".to_string(),
                    "AVRO - gzip".to_string(),
                    "AVRO - snappy".to_string(),
                    "AVRO - lz4".to_string(),
                ]),
            ),
            config_field(
                "s3.compression.level",
                "S3 compression level",
                "int",
                false,
                None,
            ),
            config_field("s3.part.size", "S3 part size in bytes", "long", false, None),
            config_field(
                "path.format",
                "Path format for S3 objects",
                "string",
                false,
                None,
            ),
        ],
        sensitive_configs: vec![
            "aws.access.key.id".to_string(),
            "aws.secret.access.key".to_string(),
        ],
    }
}

pub(crate) fn snowflake_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "SnowflakeSink".to_string(),
        display_name: "Snowflake Sink".to_string(),
        connector_class: "SnowflakeSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Snowflake".to_string(),
        required_configs: vec![
            config_field("snowflake.url", "Snowflake URL", "string", true, None),
            config_field(
                "snowflake.username",
                "Snowflake username",
                "string",
                true,
                None,
            ),
            config_field(
                "snowflake.database",
                "Snowflake database",
                "string",
                true,
                None,
            ),
            config_field("snowflake.schema", "Snowflake schema", "string", true, None),
            config_field("snowflake.table", "Snowflake table", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "snowflake.password",
                "Snowflake password",
                "string",
                false,
                None,
            ),
            config_field(
                "snowflake.private.key",
                "Snowflake private key",
                "string",
                false,
                None,
            ),
            config_field(
                "snowflake.warehouse",
                "Snowflake warehouse",
                "string",
                false,
                None,
            ),
            config_field("snowflake.role", "Snowflake role", "string", false, None),
            config_field(
                "snowflake.insert.mode",
                "Snowflake insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "snowflake.pk.mode",
                "Snowflake primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "snowflake.pk.fields",
                "Snowflake primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "snowflake.auto.create",
                "Snowflake auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "snowflake.auto.evolve",
                "Snowflake auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
        ],
        sensitive_configs: vec![
            "snowflake.password".to_string(),
            "snowflake.private.key".to_string(),
        ],
    }
}

pub(crate) fn postgresql_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "PostgresSink".to_string(),
        display_name: "PostgreSQL Sink (JDBC)".to_string(),
        connector_class: "PostgresSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to PostgreSQL using JDBC".to_string(),
        required_configs: vec![
            config_field("connection.host", "Database hostname", "string", true, None),
            config_field("connection.port", "Database port", "int", true, None),
            config_field("connection.user", "Database username", "string", true, None),
            config_field("db.name", "Database name", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "ssl.mode",
                "SSL mode for database connection",
                "string",
                false,
                Some(vec![
                    "disable".to_string(),
                    "prefer".to_string(),
                    "require".to_string(),
                    "verify-ca".to_string(),
                    "verify-full".to_string(),
                ]),
            ),
            config_field(
                "insert.mode",
                "Insert mode for records",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "INSERT".to_string(),
                    "upsert".to_string(),
                    "UPSERT".to_string(),
                    "update".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "table.name.format",
                "Table name format",
                "string",
                false,
                None,
            ),
            config_field(
                "pk.mode",
                "Primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field("pk.fields", "Primary key fields", "string", false, None),
            config_field(
                "auto.create",
                "Auto-create tables",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "auto.evolve",
                "Auto-evolve table schemas",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field("batch.size", "Batch size for inserts", "int", false, None),
            config_field(
                "max.poll.records",
                "Maximum number of records to poll",
                "int",
                false,
                None,
            ),
            config_field("db.timezone", "Database timezone", "string", false, None),
        ],
        sensitive_configs: vec!["connection.password".to_string()],
    }
}

pub(crate) fn mysql_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "MySQLSink".to_string(),
        display_name: "MySQL Sink (JDBC)".to_string(),
        connector_class: "MySQLSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to MySQL using JDBC".to_string(),
        required_configs: vec![
            config_field("connection.host", "Database hostname", "string", true, None),
            config_field("connection.port", "Database port", "int", true, None),
            config_field("connection.user", "Database username", "string", true, None),
            config_field("db.name", "Database name", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "ssl.mode",
                "SSL mode for database connection",
                "string",
                false,
                Some(vec![
                    "disabled".to_string(),
                    "preferred".to_string(),
                    "required".to_string(),
                    "verify-ca".to_string(),
                    "verify-identity".to_string(),
                ]),
            ),
            config_field(
                "insert.mode",
                "Insert mode for records",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "INSERT".to_string(),
                    "upsert".to_string(),
                    "UPSERT".to_string(),
                    "update".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "table.name.format",
                "Table name format",
                "string",
                false,
                None,
            ),
            config_field(
                "pk.mode",
                "Primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field("pk.fields", "Primary key fields", "string", false, None),
            config_field(
                "auto.create",
                "Auto-create tables",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "auto.evolve",
                "Auto-evolve table schemas",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field("batch.size", "Batch size for inserts", "int", false, None),
            config_field(
                "max.poll.records",
                "Maximum number of records to poll",
                "int",
                false,
                None,
            ),
            config_field("db.timezone", "Database timezone", "string", false, None),
        ],
        sensitive_configs: vec!["connection.password".to_string()],
    }
}

pub(crate) fn microsoft_sql_server_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "MicrosoftSqlServerSink".to_string(),
        display_name: "Microsoft SQL Server Sink (JDBC)".to_string(),
        connector_class: "MicrosoftSqlServerSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Microsoft SQL Server using JDBC".to_string(),
        required_configs: vec![
            config_field("connection.host", "Database hostname", "string", true, None),
            config_field("connection.port", "Database port", "int", true, None),
            config_field("connection.user", "Database username", "string", true, None),
            config_field("db.name", "Database name", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "connection.password",
                "Database password",
                "string",
                false,
                None,
            ),
            config_field(
                "connection.sslmode",
                "SSL mode for database connection",
                "string",
                false,
                Some(vec![
                    "disable".to_string(),
                    "require".to_string(),
                    "verify-ca".to_string(),
                    "verify-full".to_string(),
                    "prefer".to_string(),
                ]),
            ),
            config_field(
                "insert.mode",
                "Insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "pk.mode",
                "Primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field("pk.fields", "Primary key fields", "string", false, None),
            config_field(
                "auto.create",
                "Auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "auto.evolve",
                "Auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
        ],
        sensitive_configs: vec!["connection.password".to_string()],
    }
}

pub(crate) fn oracle_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "OracleSink".to_string(),
        display_name: "Oracle Sink (JDBC)".to_string(),
        connector_class: "OracleSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Oracle Database using JDBC".to_string(),
        required_configs: vec![
            config_field("connection.host", "Database hostname", "string", true, None),
            config_field("connection.port", "Database port", "int", true, None),
            config_field("connection.user", "Database username", "string", true, None),
            config_field("db.name", "Database name", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "connection.password",
                "Database password",
                "string",
                false,
                None,
            ),
            config_field(
                "connection.sslmode",
                "SSL mode for database connection",
                "string",
                false,
                Some(vec![
                    "disable".to_string(),
                    "require".to_string(),
                    "verify-ca".to_string(),
                    "verify-full".to_string(),
                    "prefer".to_string(),
                ]),
            ),
            config_field(
                "insert.mode",
                "Insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "pk.mode",
                "Primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field("pk.fields", "Primary key fields", "string", false, None),
            config_field(
                "auto.create",
                "Auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "auto.evolve",
                "Auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
        ],
        sensitive_configs: vec!["connection.password".to_string()],
    }
}

pub(crate) fn mongodb_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "MongoDBSink".to_string(),
        display_name: "MongoDB Sink".to_string(),
        connector_class: "MongoDBSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to MongoDB".to_string(),
        required_configs: vec![
            config_field("mongodb.host", "MongoDB hostname", "string", true, None),
            config_field("mongodb.port", "MongoDB port", "int", true, None),
            config_field("mongodb.username", "MongoDB username", "string", true, None),
            config_field(
                "mongodb.database",
                "MongoDB database name",
                "string",
                true,
                None,
            ),
            config_field(
                "mongodb.collection",
                "MongoDB collection name",
                "string",
                true,
                None,
            ),
        ],
        optional_configs: vec![
            config_field(
                "mongodb.password",
                "MongoDB password",
                "string",
                false,
                None,
            ),
            config_field(
                "mongodb.auth.source",
                "MongoDB authentication source",
                "string",
                false,
                None,
            ),
            config_field(
                "mongodb.insert.mode",
                "MongoDB insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "mongodb.pk.mode",
                "MongoDB primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "mongodb.pk.fields",
                "MongoDB primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "mongodb.ssl.enabled",
                "Enable SSL",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
        ],
        sensitive_configs: vec!["mongodb.password".to_string()],
    }
}

pub(crate) fn elasticsearch_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "ElasticsearchSink".to_string(),
        display_name: "Elasticsearch Sink".to_string(),
        connector_class: "ElasticsearchSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Elasticsearch".to_string(),
        required_configs: vec![
            config_field(
                "elasticsearch.host",
                "Elasticsearch hostname",
                "string",
                true,
                None,
            ),
            config_field(
                "elasticsearch.port",
                "Elasticsearch port",
                "int",
                true,
                None,
            ),
            config_field(
                "elasticsearch.index",
                "Elasticsearch index",
                "string",
                true,
                None,
            ),
        ],
        optional_configs: vec![
            config_field(
                "elasticsearch.username",
                "Elasticsearch username",
                "string",
                false,
                None,
            ),
            config_field(
                "elasticsearch.password",
                "Elasticsearch password",
                "string",
                false,
                None,
            ),
            config_field(
                "elasticsearch.insert.mode",
                "Elasticsearch insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "elasticsearch.pk.mode",
                "Elasticsearch primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "elasticsearch.pk.fields",
                "Elasticsearch primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "elasticsearch.ssl.enabled",
                "Enable SSL",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "elasticsearch.batch.size",
                "Elasticsearch batch size",
                "int",
                false,
                None,
            ),
        ],
        sensitive_configs: vec!["elasticsearch.password".to_string()],
    }
}

pub(crate) fn bigquery_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "BigQuerySink".to_string(),
        display_name: "Google BigQuery Sink".to_string(),
        connector_class: "BigQuerySink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Google BigQuery".to_string(),
        required_configs: vec![
            config_field(
                "gcp.project.id",
                "Google Cloud project ID",
                "string",
                true,
                None,
            ),
            config_field(
                "gcp.dataset",
                "Google BigQuery dataset",
                "string",
                true,
                None,
            ),
            config_field("gcp.table", "Google BigQuery table", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "gcp.credentials.json",
                "Google Cloud credentials JSON",
                "string",
                false,
                None,
            ),
            config_field(
                "gcp.insert.mode",
                "Google BigQuery insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "gcp.pk.mode",
                "Google BigQuery primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "gcp.pk.fields",
                "Google BigQuery primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "gcp.auto.create",
                "Google BigQuery auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "gcp.auto.evolve",
                "Google BigQuery auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "gcp.batch.size",
                "Google BigQuery batch size",
                "int",
                false,
                None,
            ),
        ],
        sensitive_configs: vec!["gcp.credentials.json".to_string()],
    }
}

pub(crate) fn redshift_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "RedshiftSink".to_string(),
        display_name: "Amazon Redshift Sink".to_string(),
        connector_class: "RedshiftSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Amazon Redshift".to_string(),
        required_configs: vec![
            config_field("redshift.host", "Redshift hostname", "string", true, None),
            config_field("redshift.port", "Redshift port", "int", true, None),
            config_field(
                "redshift.username",
                "Redshift username",
                "string",
                true,
                None,
            ),
            config_field(
                "redshift.database",
                "Redshift database",
                "string",
                true,
                None,
            ),
            config_field("redshift.table", "Redshift table", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "redshift.password",
                "Redshift password",
                "string",
                false,
                None,
            ),
            config_field(
                "redshift.insert.mode",
                "Redshift insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "redshift.pk.mode",
                "Redshift primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "redshift.pk.fields",
                "Redshift primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "redshift.auto.create",
                "Redshift auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "redshift.auto.evolve",
                "Redshift auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "redshift.batch.size",
                "Redshift batch size",
                "int",
                false,
                None,
            ),
        ],
        sensitive_configs: vec!["redshift.password".to_string()],
    }
}

pub(crate) fn databricks_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "DatabricksSink".to_string(),
        display_name: "Databricks Sink".to_string(),
        connector_class: "DatabricksSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Databricks".to_string(),
        required_configs: vec![
            config_field(
                "databricks.host",
                "Databricks hostname",
                "string",
                true,
                None,
            ),
            config_field("databricks.port", "Databricks port", "int", true, None),
            config_field(
                "databricks.username",
                "Databricks username",
                "string",
                true,
                None,
            ),
            config_field(
                "databricks.database",
                "Databricks database",
                "string",
                true,
                None,
            ),
            config_field("databricks.table", "Databricks table", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "databricks.password",
                "Databricks password",
                "string",
                false,
                None,
            ),
            config_field(
                "databricks.insert.mode",
                "Databricks insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "databricks.pk.mode",
                "Databricks primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field(
                "databricks.pk.fields",
                "Databricks primary key fields",
                "string",
                false,
                None,
            ),
            config_field(
                "databricks.auto.create",
                "Databricks auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "databricks.auto.evolve",
                "Databricks auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "databricks.batch.size",
                "Databricks batch size",
                "int",
                false,
                None,
            ),
        ],
        sensitive_configs: vec!["databricks.password".to_string()],
    }
}

pub(crate) fn jdbc_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "JdbcSinkConnector".to_string(),
        display_name: "JDBC Sink (Generic)".to_string(),
        connector_class: "JdbcSinkConnector".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to any JDBC-compatible database".to_string(),
        required_configs: vec![
            config_field(
                "connection.url",
                "JDBC connection URL",
                "string",
                true,
                None,
            ),
            config_field("connection.user", "Database username", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "connection.password",
                "Database password",
                "string",
                false,
                None,
            ),
            config_field(
                "insert.mode",
                "Insert mode",
                "string",
                false,
                Some(vec![
                    "insert".to_string(),
                    "upsert".to_string(),
                    "update".to_string(),
                    "INSERT".to_string(),
                    "UPSERT".to_string(),
                    "UPDATE".to_string(),
                ]),
            ),
            config_field(
                "pk.mode",
                "Primary key mode",
                "string",
                false,
                Some(vec![
                    "none".to_string(),
                    "kafka".to_string(),
                    "record_key".to_string(),
                    "record_value".to_string(),
                ]),
            ),
            config_field("pk.fields", "Primary key fields", "string", false, None),
            config_field(
                "auto.create",
                "Auto create",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "auto.evolve",
                "Auto evolve",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field("batch.size", "Batch size", "int", false, None),
        ],
        sensitive_configs: vec!["connection.password".to_string()],
    }
}

pub(crate) fn splunk_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "SplunkSink".to_string(),
        display_name: "Splunk Sink".to_string(),
        connector_class: "SplunkSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to Splunk".to_string(),
        required_configs: vec![
            config_field("splunk.hec.uri", "Splunk HEC URI", "string", true, None),
            config_field("splunk.hec.token", "Splunk HEC token", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "splunk.hec.ssl.validate.certs",
                "Validate SSL certificates",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "splunk.hec.ssl.trust.store.path",
                "SSL trust store path",
                "string",
                false,
                None,
            ),
            config_field(
                "splunk.hec.ssl.trust.store.password",
                "SSL trust store password",
                "string",
                false,
                None,
            ),
            config_field(
                "splunk.hec.ssl.key.store.path",
                "SSL key store path",
                "string",
                false,
                None,
            ),
            config_field(
                "splunk.hec.ssl.key.store.password",
                "SSL key store password",
                "string",
                false,
                None,
            ),
            config_field(
                "splunk.hec.ssl.key.store.key.password",
                "SSL key store key password",
                "string",
                false,
                None,
            ),
            config_field(
                "splunk.hec.batch.size",
                "Batch size for events",
                "int",
                false,
                None,
            ),
            config_field(
                "splunk.hec.max.batch.size",
                "Maximum batch size",
                "int",
                false,
                None,
            ),
            config_field(
                "splunk.hec.ack.enabled",
                "Enable acknowledgments",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "splunk.hec.raw",
                "Send raw data",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
        ],
        sensitive_configs: vec![
            "splunk.hec.token".to_string(),
            "splunk.hec.ssl.trust.store.password".to_string(),
            "splunk.hec.ssl.key.store.password".to_string(),
            "splunk.hec.ssl.key.store.key.password".to_string(),
        ],
    }
}

pub(crate) fn clickhouse_sink() -> ConnectorDefinition {
    ConnectorDefinition {
        name: "ClickHouseSink".to_string(),
        display_name: "ClickHouse Sink".to_string(),
        connector_class: "ClickHouseSink".to_string(),
        connector_type: ConnectorType::Sink,
        description: "Write data from Kafka topics to ClickHouse".to_string(),
        required_configs: vec![
            config_field("hostname", "ClickHouse hostname", "string", true, None),
            config_field("port", "ClickHouse port", "int", true, None),
            config_field("username", "ClickHouse username", "string", true, None),
            config_field("database", "ClickHouse database name", "string", true, None),
        ],
        optional_configs: vec![
            config_field(
                "ssl",
                "Enable SSL connection",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "max.poll.records",
                "Maximum number of records to poll",
                "int",
                false,
                None,
            ),
            config_field(
                "consumer.override.auto.offset.reset",
                "Auto offset reset strategy",
                "string",
                false,
                Some(vec!["earliest".to_string(), "latest".to_string()]),
            ),
            config_field(
                "topic2TableMap",
                "Topic to table mapping",
                "string",
                false,
                None,
            ),
            config_field("tasks.max", "Maximum number of tasks", "int", false, None),
            config_field(
                "errors.tolerance",
                "Error tolerance",
                "string",
                false,
                Some(vec!["all".to_string(), "none".to_string()]),
            ),
            config_field(
                "bypassRowBinary",
                "Bypass RowBinary format",
                "boolean",
                false,
                Some(vec!["true".to_string(), "false".to_string()]),
            ),
            config_field(
                "input.data.format",
                "Input data format",
                "string",
                false,
                Some(vec![
                    "AVRO".to_string(),
                    "JSON".to_string(),
                    "PROTOBUF".to_string(),
                    "local.schema_formats.avro".to_string(),
                    "local.schema_formats.json".to_string(),
                    "local.schema_formats.protobuf".to_string(),
                ]),
            ),
            config_field(
                "transforms",
                "Transformations to apply",
                "string",
                false,
                None,
            ),
        ],
        sensitive_configs: vec!["password".to_string()],
    }
}
